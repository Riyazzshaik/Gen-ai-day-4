{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSBuwtNoQq6H",
        "outputId": "1c0bc377-87dc-4587-a291-9dfc6f6ef1da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_x8FCpPxRcjg",
        "outputId": "709f4469-3b11-41bc-84a4-84ca3dee348c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text = \"Artificial Intelligence is transforming industries. NLP is a key part of AI. Tokenization is the first NLP step.\"\n",
        "print(sent_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DA5xQ5ZpSBw3",
        "outputId": "8da31725-d3ae-4b8b-90e3-692cd2878cef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Artificial Intelligence is transforming industries.', 'NLP is a key part of AI.', 'Tokenization is the first NLP step.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"Artificial Intelligence is transforming industries. NLP is a key part of AI. Tokenization is the first NLP step.\"\n",
        "print(word_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhCTl1qRSLuS",
        "outputId": "bbd5c7b3-41e3-4850-98ee-b3ee92773749"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Artificial', 'Intelligence', 'is', 'transforming', 'industries', '.', 'NLP', 'is', 'a', 'key', 'part', 'of', 'AI', '.', 'Tokenization', 'is', 'the', 'first', 'NLP', 'step', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hATJgtAqSTPm",
        "outputId": "b818b466-c623-4e7d-d43b-a8e6ee337aa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "words = [\"playing\", \"played\", \"plays\", \"happily\", \"studies\"]\n",
        "\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "print(stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFEVytD0SZWT",
        "outputId": "3490cc63-62e9-4045-9e6b-6131dffc5272"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['play', 'play', 'play', 'happili', 'studi']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = [\"running\", \"cars\", \"better\", \"children\", \"feet\"]\n",
        "\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "print(lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBql4fZCSr4t",
        "outputId": "c0414f76-d46f-4bdb-98fc-2a992de4097c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['running', 'car', 'better', 'child', 'foot']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "words = [\"running\", \"played\", \"plays\", \"happily\", \"studies\"]\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "print(f\"Word :{words} | Stem : {stemmed_words} | leamma :{lemmatized_words}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pr7LGIlJTfKM",
        "outputId": "3f739edf-da62-4e68-9c78-1052b8986b1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word :['running', 'played', 'plays', 'happily', 'studies'] | Stem : ['run', 'play', 'play', 'happili', 'studi'] | leamma :['running', 'played', 'play', 'happily', 'study']\n"
          ]
        }
      ]
    }
  ]
}